"""
Grid Search + Model Kaydetme
- Hassas parametrelerde ince ayar
- En iyi modeli kaydet
- Reproducibility iÃ§in tÃ¼m bilgileri kaydet
"""

import pandas as pd
import numpy as np
import json
import joblib
from pathlib import Path
import time
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AYARLAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TRAIN_FILE = "/home/azureuser/IDS_Project/test/train_selected.csv"
TEST_FILE = "/home/azureuser/IDS_Project/test/test_selected.csv"
RESULTS_DIR = Path("results")
REPORTS_DIR = Path("reports")
MODELS_DIR = Path("models")  # Model klasÃ¶rÃ¼
TARGET_COLUMN = " Label"
RANDOM_STATE = 42
N_FOLDS = 5

RESULTS_DIR.mkdir(exist_ok=True)
REPORTS_DIR.mkdir(exist_ok=True)
MODELS_DIR.mkdir(exist_ok=True)  # Model klasÃ¶rÃ¼ oluÅŸtur

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRID PARAMETRELERÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
param_grid = {
    'n_estimators': [150],
    'max_depth': [19, 20, 21],
    'min_samples_split':  [2, 3],
    'min_samples_leaf': [1],
    'max_features': ['log2'],
    'class_weight': [None]
}

total_combinations = np.prod([len(v) for v in param_grid.values()])

print("=" * 70)
print("   GRID SEARCH + MODEL KAYDETME")
print("=" * 70)
print(f"\nTarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"\nStrateji:  Sadece hassas parametrelerde ince ayar")
print(f"\nSabit parametreler (RandomSearch'ten):")
print(f"  - n_estimators:      150")
print(f"  - max_features:     log2")
print(f"  - class_weight:     None")
print(f"  - min_samples_leaf: 1")

print(f"\nFine-tune edilecek:")
print(f"  - max_depth:        {param_grid['max_depth']}")
print(f"  - min_samples_split: {param_grid['min_samples_split']}")

print(f"\nToplam kombinasyon:    {total_combinations}")
print(f"Tahmini sÃ¼re:        ~{total_combinations * N_FOLDS * 3:.0f} dakika ({total_combinations * N_FOLDS * 3 / 60:.1f} saat)")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VERÄ° YÃœKLEME
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ“‚ Veri yÃ¼kleniyor...")
df_train = pd.read_csv(TRAIN_FILE)
df_test = pd.read_csv(TEST_FILE)

X_train = df_train.drop(columns=[TARGET_COLUMN])
y_train = df_train[TARGET_COLUMN].squeeze()
X_test = df_test.drop(columns=[TARGET_COLUMN])
y_test = df_test[TARGET_COLUMN].squeeze()

print(f"âœ“ Train:   {len(X_train):,} satÄ±r, {len(X_train.columns)} feature")
print(f"âœ“ Test:   {len(X_test):,} satÄ±r")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BASELINE (RandomSearch En Ä°yi)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   BASELINE (RandomSearch En Ä°yi)")
print("=" * 70)

baseline_params = {
    'n_estimators': 150,
    'max_depth': 20,
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'max_features':  'log2',
    'class_weight': None,
    'random_state':  RANDOM_STATE,
    'n_jobs': -1
}

print("\nâ³ Baseline eÄŸitiliyor...")
baseline_start = time.time()

baseline = RandomForestClassifier(**baseline_params)
baseline.fit(X_train, y_train)
y_pred_baseline = baseline.predict(X_test)

baseline_time = time.time() - baseline_start

baseline_f1 = f1_score(y_test, y_pred_baseline, average='macro')
baseline_recall = recall_score(y_test, y_pred_baseline, average='macro')
baseline_precision = precision_score(y_test, y_pred_baseline, average='macro')
baseline_accuracy = accuracy_score(y_test, y_pred_baseline)

print(f"âœ“ TamamlandÄ± ({baseline_time:.1f}s)")
print(f"\nRandomSearch En Ä°yi Test SonuÃ§larÄ±:")
print(f"  F1-macro:    {baseline_f1:.6f}")
print(f"  Recall:     {baseline_recall:.6f}")
print(f"  Precision:  {baseline_precision:.6f}")
print(f"  Accuracy:   {baseline_accuracy:.6f}")

# Baseline modeli kaydet
baseline_model_file = MODELS_DIR / "baseline_random_search_model.pkl"
joblib.dump(baseline, baseline_model_file)
print(f"\nâœ“ Baseline model kaydedildi: {baseline_model_file}")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRID SEARCH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   GRID SEARCH")
print("=" * 70)

grid_search = GridSearchCV(
    estimator=RandomForestClassifier(
        n_estimators=150,
        min_samples_leaf=1,
        max_features='log2',
        class_weight=None,
        random_state=RANDOM_STATE,
        n_jobs=1
    ),
    param_grid={
        'max_depth': param_grid['max_depth'],
        'min_samples_split':  param_grid['min_samples_split']
    },
    cv=N_FOLDS,
    scoring='f1_macro',
    n_jobs=-1,
    verbose=2,
    return_train_score=True
)

print("\nâ³ GridSearchCV baÅŸladÄ±...")
print("-" * 70)

search_start = time.time()
grid_search.fit(X_train, y_train)
search_time = time.time() - search_start

print("-" * 70)
print(f"âœ“ TamamlandÄ± ({search_time/60:.1f} dakika)")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EN Ä°YÄ° MODEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   EN Ä°YÄ° MODEL")
print("=" * 70)

best_params = grid_search.best_params_
best_cv_score = grid_search.best_score_
best_model = grid_search.best_estimator_

print(f"\nğŸ“Š GridSearch En Ä°yi CV F1: {best_cv_score:.6f}")
print(f"ğŸ“Š RandomSearch'ten iyileÅŸme: {(best_cv_score - 0.993738)*100:+.4f}%")

print(f"\nâœ… En Ä°yi Parametreler:")
print("-" * 70)
print(f"  max_depth:           {best_params['max_depth']}")
print(f"  min_samples_split:  {best_params['min_samples_split']}")
print(f"  n_estimators:       150 (sabit)")
print(f"  min_samples_leaf:   1 (sabit)")
print(f"  max_features:       log2 (sabit)")
print(f"  class_weight:       None (sabit)")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST DEÄERLENDÄ°RME
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   TEST SETÄ° DEÄERLENDÄ°RME")
print("=" * 70)

y_pred_tuned = best_model.predict(X_test)

tuned_f1 = f1_score(y_test, y_pred_tuned, average='macro')
tuned_recall = recall_score(y_test, y_pred_tuned, average='macro')
tuned_precision = precision_score(y_test, y_pred_tuned, average='macro')
tuned_accuracy = accuracy_score(y_test, y_pred_tuned)

print(f"\nğŸ“Š KarÅŸÄ±laÅŸtÄ±rma:")
print("-" * 70)
print(f"{'Metrik':<15} {'RandomSearch':<15} {'GridSearch':<15} {'Ä°yileÅŸme':<12}")
print("-" * 70)
print(f"{'F1-macro':<15} {baseline_f1:<15.6f} {tuned_f1:<15.6f} {(tuned_f1-baseline_f1)*100:+.4f}%")
print(f"{'Recall':<15} {baseline_recall:<15.6f} {tuned_recall:<15.6f} {(tuned_recall-baseline_recall)*100:+.4f}%")
print(f"{'Precision':<15} {baseline_precision:<15.6f} {tuned_precision:<15.6f} {(tuned_precision-baseline_precision)*100:+.4f}%")
print(f"{'Accuracy':<15} {baseline_accuracy:<15.6f} {tuned_accuracy:<15.6f} {(tuned_accuracy-baseline_accuracy)*100:+.4f}%")

improvement = (tuned_f1 - baseline_f1) * 100
print()
if improvement > 0.01:
    print(f"âœ… AnlamlÄ± iyileÅŸme: +{improvement:.4f}%")
    decision = "GridSearch kazandÄ±!  Bu modeli kullan."
elif improvement > 0:
    print(f"âš ï¸  Minimal iyileÅŸme: +{improvement:.4f}%")
    decision = "GridSearch minimal fayda saÄŸladÄ±."
else:
    print(f"âŒ Ä°yileÅŸme yok: {improvement:+.4f}%")
    decision = "RandomSearch yeterliydi."

print(f"\nğŸ“‹ Classification Report (GridSearch):")
print("-" * 70)
print(classification_report(y_test, y_pred_tuned, digits=4))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL KAYDETME
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   MODEL KAYDETME")
print("=" * 70)

# âœ… 1. GridSearch en iyi modeli
grid_model_file = MODELS_DIR / "grid_search_best_model.pkl"
joblib.dump(best_model, grid_model_file)
print(f"\nâœ“ GridSearch en iyi model:  {grid_model_file}")

# âœ… 2. TÃ¼m GridSearchCV objesi (tÃ¼m kombinasyonlar)
grid_cv_file = MODELS_DIR / "grid_search_cv_object.pkl"
joblib.dump(grid_search, grid_cv_file)
print(f"âœ“ GridSearchCV objesi:       {grid_cv_file}")

# âœ… 3. Final model seÃ§imi (en iyi olan)
if improvement > 0:
    final_model = best_model
    final_model_source = "GridSearch"
    final_f1 = tuned_f1
else:
    final_model = baseline
    final_model_source = "RandomSearch (GridSearch iyileÅŸtirme saÄŸlamadÄ±)"
    final_f1 = baseline_f1

final_model_file = MODELS_DIR / "final_model.pkl"
joblib.dump(final_model, final_model_file)
print(f"âœ“ Final model:               {final_model_file}")
print(f"  Kaynak:  {final_model_source}")
print(f"  Test F1: {final_f1:.6f}")

# âœ… 4. Model metadata
model_metadata = {
    "model_file": str(final_model_file),
    "source": final_model_source,
    "date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    "test_f1_macro": float(final_f1),
    "parameters": final_model.get_params(),
    "feature_count": len(X_train.columns),
    "train_samples": len(X_train),
    "test_samples": len(X_test)
}

metadata_file = MODELS_DIR / "final_model_metadata.json"
with open(metadata_file, 'w') as f:
    json.dump(model_metadata, f, indent=2)
print(f"âœ“ Model metadata:           {metadata_file}")

print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TÃœM KOMBÄ°NASYONLAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   TÃœM KOMBÄ°NASYONLAR")
print("=" * 70)

cv_results = pd.DataFrame(grid_search.cv_results_)
cv_results_sorted = cv_results.sort_values('rank_test_score')

print(f"\n{'Rank':<6} {'max_depth':<12} {'min_split':<12} {'CV F1':<14} {'Std':<10}")
print("-" * 70)
for _, row in cv_results_sorted.iterrows():
    print(f"{int(row['rank_test_score']):<6} "
          f"{str(row['param_max_depth']):<12} "
          f"{str(row['param_min_samples_split']):<12} "
          f"{row['mean_test_score']:.6f}      "
          f"Â±{row['std_test_score']:.6f}")

print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SONUÃ‡LARI KAYDET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   SONUÃ‡LARI KAYDET")
print("=" * 70)

results = {
    "experiment":  "Grid Search - Ultra Minimal",
    "date":  datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    "strategy": "Fine-tune only max_depth and min_samples_split",
    
    "grid_config": {
        "total_combinations": int(total_combinations),
        "total_fits": int(total_combinations * N_FOLDS),
        "search_time_minutes": round(search_time / 60, 2),
        "avg_time_per_fit_minutes": round(search_time / (total_combinations * N_FOLDS), 2)
    },
    
    "baseline_random_search": {
        "cv_f1": 0.993738,
        "test_f1_macro": float(baseline_f1),
        "test_recall": float(baseline_recall),
        "test_precision": float(baseline_precision),
        "test_accuracy": float(baseline_accuracy),
        "params": baseline_params,
        "model_file": str(baseline_model_file)
    },
    
    "grid_search_best":  {
        "cv_f1": float(best_cv_score),
        "test_f1_macro": float(tuned_f1),
        "test_recall": float(tuned_recall),
        "test_precision": float(tuned_precision),
        "test_accuracy": float(tuned_accuracy),
        "params": {
            "max_depth": int(best_params['max_depth']),
            "min_samples_split": int(best_params['min_samples_split']),
            "n_estimators":  150,
            "min_samples_leaf": 1,
            "max_features": "log2",
            "class_weight": None
        },
        "improvement_cv_%": float((best_cv_score - 0.993738) * 100),
        "improvement_test_%": float((tuned_f1 - baseline_f1) * 100),
        "model_file": str(grid_model_file)
    },
    
    "final_model": {
        "source": final_model_source,
        "test_f1_macro": float(final_f1),
        "model_file": str(final_model_file),
        "metadata_file": str(metadata_file),
        "decision": decision
    },
    
    "all_combinations": [
        {
            "rank": int(row['rank_test_score']),
            "max_depth": int(row['param_max_depth']),
            "min_samples_split": int(row['param_min_samples_split']),
            "cv_f1_mean": float(row['mean_test_score']),
            "cv_f1_std": float(row['std_test_score'])
        }
        for _, row in cv_results_sorted.iterrows()
    ],
    
    "files":  {
        "baseline_model": str(baseline_model_file),
        "grid_best_model": str(grid_model_file),
        "grid_cv_object": str(grid_cv_file),
        "final_model": str(final_model_file),
        "final_metadata": str(metadata_file)
    }
}

json_file = RESULTS_DIR / "grid_search_minimal_results.json"
with open(json_file, 'w') as f:
    json.dump(results, f, indent=2)
print(f"\nâœ“ {json_file}")

# CSV
csv_file = RESULTS_DIR / "grid_search_cv_details.csv"
cv_results.to_csv(csv_file, index=False)
print(f"âœ“ {csv_file}")

print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ã–ZET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("   Ã–ZET")
print("=" * 70)
print(f"âœ“ {total_combinations} kombinasyon Ã— {N_FOLDS}-fold = {total_combinations * N_FOLDS} fit")
print(f"âœ“ GerÃ§ek sÃ¼re: {search_time/60:.1f} dakika")
print(f"âœ“ Ortalama fit sÃ¼resi: {search_time/(total_combinations * N_FOLDS):.2f} dakika")
print(f"âœ“ Test F1 iyileÅŸme: {(tuned_f1-baseline_f1)*100:+.4f}%")

print(f"\nğŸ“ Kaydedilen dosyalar:")
print("-" * 70)
print(f"Modeller:")
print(f"  â”œâ”€ {baseline_model_file.name}")
print(f"  â”œâ”€ {grid_model_file.name}")
print(f"  â”œâ”€ {grid_cv_file.name}")
print(f"  â””â”€ {final_model_file.name} â­ (kullanÄ±lacak)")

print(f"\nRaporlar:")
print(f"  â”œâ”€ {json_file.name}")
print(f"  â”œâ”€ {csv_file.name}")
print(f"  â””â”€ {metadata_file.name}")

print(f"\nğŸ¯ Karar:  {decision}")
print(f"ğŸ¯ Final Model: {final_model_file}")
print(f"ğŸ¯ Test F1-macro: {final_f1:.6f}")
print("=" * 70)